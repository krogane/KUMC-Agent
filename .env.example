# Discord / API keys
DISCORD_BOT_TOKEN=
GEMINI_API_KEY=

# Google Drive source data
FOLDER_ID=
GOOGLE_APPLICATION_CREDENTIALS=.secrets/google-sa.json
GOOGLE_SERVICE_ACCOUNT_FILE=
DRIVE_MAX_FILES=0 # 0 or unset: no limit

# LLM settings
LLM_PROVIDER=llama
GEMINI_MODEL=gemini-3-flash-preview
LLAMA_MODEL_PATH=app/model/qwen2.5-1.5b-instruct-q4_k_m.gguf
COMMAND_PREFIX=/ai
TEMPERATURE=0.0
MAX_OUTPUT_TOKENS=512
THINKING_LEVEL=minimal
TOP_K=5
RAPTOR_SEARCH_TOP_K=20
KEYWORD_SEARCH_TOP_K=20
PARENT_DOC_ENABLED=1

# Embeddings (offline path recommended)
EMBEDDING_MODEL=app/model/embedding/
RAPTOR_EMBEDDING_MODEL= # optional override for RAPTOR clustering
CROSS_ENCODER_MODEL=

# Chunking (pre-embedding)
REC_CHUNK_SIZE=400
REC_CHUNK_OVERLAP=80
REC_MIN_CHUNK_TOKENS=0

# llama.cpp runtime
LLAMA_CTX_SIZE=1024
LLAMA_GPU_LAYERS=0 # GPU推論: 1以上, CPU推論: 0
LLAMA_THREADS=4

# Index build cleanup flags
CLEAR_RAW_DATA=0
CLEAR_REC_CHUNK_DATA=0
CLEAR_LLM_CHUNK_DATA=0
CLEAR_PROP_CHUNK_DATA=0
CLEAR_RAPTOR_CHUNK_DATA=0

# LLM chunking defaults
LLM_CHUNK_ENABLED=1
LLM_CHUNK_PROVIDER=llama
LLM_CHUNK_MODEL=gemini-3-flash-preview
LLM_CHUNK_LLAMA_MODEL_PATH=app/model/gemma-3n-E2B-it-Q4_K_M.gguf
LLM_CHUNK_LLAMA_CTX_SIZE=1024
LLM_CHUNK_TEMPERATURE=0.0
LLM_CHUNK_MAX_OUTPUT_TOKENS=1024
LLM_CHUNK_SIZE=100
LLM_CHUNK_MAX_RETRIES=2

# Proposition chunking
PROP_CHUNK_ENABLED=0
PROP_CHUNK_PROVIDER=llama
PROP_CHUNK_LLAMA_CTX_SIZE=1024
PROP_CHUNK_TEMPERATURE=0.0
PROP_CHUNK_MAX_OUTPUT_TOKENS=1024
PROP_CHUNK_SIZE=100
PROP_CHUNK_MAX_RETRIES=2

# RAPTOR
RAPTOR_ENABLED=0
RAPTOR_CLUSTER_MAX_TOKENS=1024
RAPTOR_SUMMARY_MAX_TOKENS=256
RAPTOR_STOP_CHUNK_COUNT=20
RAPTOR_K_MAX=8
RAPTOR_K_SELECTION=elbow
RAPTOR_SUMMARY_PROVIDER=llama
RAPTOR_SUMMARY_LLAMA_CTX_SIZE=1024
RAPTOR_SUMMARY_TEMPERATURE=0.0
RAPTOR_SUMMARY_MAX_RETRIES=3

# Python dotenv
PYTHON_DOTENV_DISABLED=0
