# Discord / API keys
DISCORD_BOT_TOKEN=
DISCORD_GUILD_ALLOW_LIST= # comma-separated guild IDs, empty = allow all
GEMINI_API_KEY=

# Google Drive source data
FOLDER_ID=
GOOGLE_APPLICATION_CREDENTIALS=.secrets/google-sa.json
GOOGLE_SERVICE_ACCOUNT_FILE=
DRIVE_MAX_FILES=0 # 0 or unset: no limit

# LLM settings (answering)
LLM_PROVIDER=llama # gemini: GeminiAPIを利用, llama: llama.cppでローカルLLMを使用
GEMINI_MODEL=gemini-3-flash-preview
LLM_MODEL_DIR=app/model/llm
WHISPER_MODEL_DIR=app/model/whisper
LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
COMMAND_PREFIX=/ai
TEMPERATURE=0.0
MAX_OUTPUT_TOKENS=1024
THINKING_LEVEL=minimal
CHAT_HISTORY_ENABLED=0 # 1: チャット履歴をプロンプトに含める, 0: 含めない
CHAT_HISTORY_MAX_TURNS=5 # 直近n件の入出力を保持
PROMPT_HISTORY_DEFAULT_TURNS=3 # 追加メモリ無効時にプロンプトへ渡す履歴ターン上限
PROMPT_HISTORY_ADDITIONAL_TURNS=10 # 追加メモリ有効時にプロンプトへ渡す履歴ターン上限
TOP_K=10 # 回答用LLMのプロンプトに入れる情報源（チャンク単位）の数
DENSE_SEARCH_TOP_K=20
SPARSE_SEARCH_TOP_K=20
SPARSE_SEARCH_ORIGINAL_TOP_K=20
SPARSE_SEARCH_TRANSFORM_TOP_K=20
PARENT_DOC_ENABLED=1
SUDACHI_MODE=B
SPARSE_BM25_K1=1.5
SPARSE_BM25_B=0.75
SPARSE_USE_NORMALIZED_FORM=1
SPARSE_REMOVE_SYMBOLS=1
SOURCE_MAX_COUNT=3
ANSWER_JSON_MAX_RETRIES=2
ANSWER_RESEARCH_MAX_RETRIES=3

# Auto indexing schedule
AUTO_INDEX_ENABLED=0 # 1: 自動Indexingを有効化
AUTO_INDEX_TIME=03:00 # 24時間表記のHH:MM
AUTO_INDEX_WEEKDAYS=mon,tue,wed,thu,fri # 0-6 or mon-sun or *

# VC meeting (voice transcription / summary)
VC_FEATURE_ENABLED=0
VC_AUTO_JOIN_ENABLED=0
VC_AUTO_JOIN_WEEKDAYS=sat
VC_AUTO_JOIN_TIME=20:00
VC_AUTO_JOIN_DURATION_MINUTES=30
VC_TARGET_VOICE_CHANNEL_NAME=例会
VC_AUTO_JOIN_MIN_PARTICIPANTS=3
VC_PARTICIPANT_CHECK_INTERVAL_SECONDS=10
VC_TRANSCRIBE_INTERVAL_SECONDS=300
VC_TRANSCRIBE_MODEL=kotoba-tech/kotoba-whisper-v2.2
VC_TRANSCRIBE_DEVICE=auto
VC_TRANSCRIBE_TORCH_DTYPE=auto
VC_TRANSCRIBE_LANGUAGE=ja
VC_AUTO_QUIT_ENABLED=1
VC_FINAL_SUMMARY_ENABLED=1
VC_SUMMARY_PREVIOUS_MAX=2
VC_SUMMARY_TARGET_CHARACTERS=100

# VC summary LLM
VC_SUMMARY_LLM_PROVIDER=llama
VC_SUMMARY_GEMINI_MODEL=gemini-3-flash-preview
VC_SUMMARY_LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
VC_SUMMARY_LLAMA_CTX_SIZE=2048
VC_SUMMARY_TEMPERATURE=0.2
VC_SUMMARY_MAX_OUTPUT_TOKENS=256
VC_SUMMARY_THINKING_LEVEL=minimal

# VC end-judge LLM
VC_END_JUDGE_LLM_PROVIDER=llama
VC_END_JUDGE_GEMINI_MODEL=gemini-3-flash-preview
VC_END_JUDGE_LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
VC_END_JUDGE_LLAMA_CTX_SIZE=2048
VC_END_JUDGE_TEMPERATURE=0.0
VC_END_JUDGE_MAX_OUTPUT_TOKENS=64
VC_END_JUDGE_THINKING_LEVEL=minimal

# VC final summary LLM
VC_FINAL_SUMMARY_LLM_PROVIDER=llama
VC_FINAL_SUMMARY_GEMINI_MODEL=gemini-3-flash-preview
VC_FINAL_SUMMARY_LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
VC_FINAL_SUMMARY_LLAMA_CTX_SIZE=2048
VC_FINAL_SUMMARY_TEMPERATURE=0.2
VC_FINAL_SUMMARY_MAX_OUTPUT_TOKENS=512
VC_FINAL_SUMMARY_THINKING_LEVEL=minimal

# Embeddings
EMBEDDING_MODEL=intfloat/multilingual-e5-small
EMBEDDING_MODEL_DIR=app/model/embedding
RAPTOR_EMBEDDING_MODEL=intfloat/multilingual-e5-small
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
CROSS_ENCODER_MODEL_DIR=app/model/cross-encoder

# First Recursive Chunking
FIRST_REC_CHUNK_SIZE=1024
FIRST_REC_CHUNK_OVERLAP=128

# Second Recursive Chunking
SECOND_REC_ENABLED=1
SECOND_REC_CHUNK_SIZE=128
SECOND_REC_CHUNK_OVERLAP=32

# Summery Chunking
SUMMERY_ENABLED=1
SUMMERY_CHARACTERS=200
SUMMERY_PROVIDER=llama
SUMMERY_GEMINI_MODEL=gemini-3-flash-preview
SUMMERY_LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
SUMMERY_LLAMA_CTX_SIZE=2048
SUMMERY_MAX_OUTPUT_TOKENS=1024
SUMMERY_TEMPERATURE=0.2
SUMMERY_MAX_RETRIES=2
SUMMERY_PROMPT=

# Proposition Chunking
PROP_ENABLED=0
PROP_PROVIDER=llama
PROP_GEMINI_MODEL=gemini-3-flash-preview
PROP_LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
PROP_LLAMA_CTX_SIZE=2048
PROP_MAX_OUTPUT_TOKENS=4096
PROP_TEMPERATURE=0.2
PROP_MAX_RETRIES=2

# RAPTOR
RAPTOR_ENABLED=0
RAPTOR_SUMMERY_PROVIDER=llama
RAPTOR_SUMMERY_GEMINI_MODEL=gemini-3-flash-preview
RAPTOR_SUMMERY_LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
RAPTOR_SUMMERY_LLAMA_CTX_SIZE=4096
RAPTOR_SUMMERY_MAX_TOKENS=256
RAPTOR_SUMMERY_TEMPERATURE=0.2
RAPTOR_SUMMERY_MAX_RETRIES=2
RAPTOR_CLUSTER_MAX_TOKENS=1024
RAPTOR_STOP_CHUNK_COUNT=20
RAPTOR_K_MAX=8
RAPTOR_K_SELECTION=elbow

# llama.cpp runtime
LLAMA_CTX_SIZE=1024
LLAMA_GPU_LAYERS=0 # GPU推論: 1以上, CPU推論: 0
LLAMA_THREADS=4 # 推論時に利用するスレッド数

# Index build cleanup flags
CLEAR_RAW_DATA=0
CLEAR_FIRST_REC_CHUNK_DATA=0
CLEAR_SECOND_REC_CHUNK_DATA=0
CLEAR_SUMMERY_CHUNK_DATA=0
CLEAR_PROP_CHUNK_DATA=0
CLEAR_RAPTOR_CHUNK_DATA=0

# Incremental update flags (0: 無効, 1: 有効)
UPDATE_RAW_DATA=1
UPDATE_FIRST_REC_CHUNK_DATA=1
UPDATE_SECOND_REC_CHUNK_DATA=1
UPDATE_SPARSE_SECOND_REC_CHUNK_DATA=1
UPDATE_SUMMERY_CHUNK_DATA=1
UPDATE_PROP_CHUNK_DATA=1
UPDATE_RAPTOR_CHUNK_DATA=1

# Python dotenv
PYTHON_DOTENV_DISABLED=0
