# Discord / API keys
DISCORD_BOT_TOKEN=
DISCORD_GUILD_ALLOW_LIST= # comma-separated guild IDs, empty = allow all
GEMINI_API_KEY=

# Google Drive source data
FOLDER_ID=
GOOGLE_APPLICATION_CREDENTIALS=.secrets/google-sa.json
GOOGLE_SERVICE_ACCOUNT_FILE=
DRIVE_MAX_FILES=0 # 0 or unset: no limit

# LLM settings (answering)
LLM_PROVIDER=llama # gemini: GeminiAPIを利用, llama: llama.cppでローカルLLMを使用
GEMINI_MODEL=gemini-3-flash-preview
LLM_MODEL_DIR=app/model/llm
LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
COMMAND_PREFIX=/ai
TEMPERATURE=0.0
MAX_OUTPUT_TOKENS=1024
THINKING_LEVEL=minimal
CHAT_HISTORY_ENABLED=0 # 1: チャット履歴をプロンプトに含める, 0: 含めない
CHAT_HISTORY_MAX_TURNS=5 # 直近n件の入出力を保持
TOP_K=10 # 回答用LLMのプロンプトに入れる情報源（チャンク単位）の数
DENSE_SEARCH_TOP_K=20
SPARSE_SEARCH_TOP_K=20
SPARSE_SEARCH_ORIGINAL_TOP_K=20
SPARSE_SEARCH_TRANSFORM_TOP_K=20
PARENT_DOC_ENABLED=1
SUDACHI_MODE=B
SPARSE_BM25_K1=1.5
SPARSE_BM25_B=0.75
SPARSE_USE_NORMALIZED_FORM=1
SPARSE_REMOVE_SYMBOLS=1
SOURCE_MAX_COUNT=3
ANSWER_JSON_MAX_RETRIES=2
ANSWER_RESEARCH_MAX_RETRIES=3

# Auto indexing schedule
AUTO_INDEX_ENABLED=0 # 1: 自動Indexingを有効化
AUTO_INDEX_TIME=03:00 # 24時間表記のHH:MM
AUTO_INDEX_WEEKDAYS=mon,tue,wed,thu,fri # 0-6 or mon-sun or *

# Embeddings
EMBEDDING_MODEL=intfloat/multilingual-e5-small
EMBEDDING_MODEL_DIR=app/model/embedding
RAPTOR_EMBEDDING_MODEL=intfloat/multilingual-e5-small
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
CROSS_ENCODER_MODEL_DIR=app/model/cross-encoder

# First Recursive Chunking
FIRST_REC_CHUNK_SIZE=1024
FIRST_REC_CHUNK_OVERLAP=128

# Second Recursive Chunking
SECOND_REC_ENABLED=1
SECOND_REC_CHUNK_SIZE=128
SECOND_REC_CHUNK_OVERLAP=32

# Summery Chunking
SUMMERY_ENABLED=1
SUMMERY_CHARACTERS=200
SUMMERY_PROVIDER=llama
SUMMERY_GEMINI_MODEL=gemini-3-flash-preview
SUMMERY_LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
SUMMERY_LLAMA_CTX_SIZE=2048
SUMMERY_MAX_OUTPUT_TOKENS=1024
SUMMERY_TEMPERATURE=0.2
SUMMERY_MAX_RETRIES=2
SUMMERY_PROMPT=

# Proposition Chunking
PROP_ENABLED=0
PROP_PROVIDER=llama
PROP_GEMINI_MODEL=gemini-3-flash-preview
PROP_LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
PROP_LLAMA_CTX_SIZE=2048
PROP_MAX_OUTPUT_TOKENS=4096
PROP_TEMPERATURE=0.2
PROP_MAX_RETRIES=2

# RAPTOR
RAPTOR_ENABLED=0
RAPTOR_SUMMERY_PROVIDER=llama
RAPTOR_SUMMERY_GEMINI_MODEL=gemini-3-flash-preview
RAPTOR_SUMMERY_LLAMA_MODEL=gemma-3n-E2B-it-IQ4_XS.gguf
RAPTOR_SUMMERY_LLAMA_CTX_SIZE=4096
RAPTOR_SUMMERY_MAX_TOKENS=256
RAPTOR_SUMMERY_TEMPERATURE=0.2
RAPTOR_SUMMERY_MAX_RETRIES=2
RAPTOR_CLUSTER_MAX_TOKENS=1024
RAPTOR_STOP_CHUNK_COUNT=20
RAPTOR_K_MAX=8
RAPTOR_K_SELECTION=elbow

# llama.cpp runtime
LLAMA_CTX_SIZE=1024
LLAMA_GPU_LAYERS=0 # GPU推論: 1以上, CPU推論: 0
LLAMA_THREADS=4 # 推論時に利用するスレッド数

# Index build cleanup flags
CLEAR_RAW_DATA=0
CLEAR_FIRST_REC_CHUNK_DATA=0
CLEAR_SECOND_REC_CHUNK_DATA=0
CLEAR_SUMMERY_CHUNK_DATA=0
CLEAR_PROP_CHUNK_DATA=0
CLEAR_RAPTOR_CHUNK_DATA=0

# Python dotenv
PYTHON_DOTENV_DISABLED=0
